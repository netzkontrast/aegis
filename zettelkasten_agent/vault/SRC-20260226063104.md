---
id: SRC-20260226063104
created: '2026-02-26T06:31:04.085213'
tags:
- '#docs'
- '#quest/writing-excellence'
- '#quest/ncp-writing-assistant'
- '#quest/documentation-improvements'
- '#quest/skill-gap-analysis'
- '#quest/style-and-meta'
- '#quest/integrate-writing-excellence'
- '#quest/system-maintenance'
- '#quest/knowledge-graph-foundation'
links: []
note_type: SRC
source_uri: docs/knowledge-extraction/08-red-phase-scenarios.md
status: processed
---

# RED Phase: Pressure Scenarios for Skill Authoring Guide

## Overview

Testing a meta-skill about skill authoring requires scenarios where skill authors are likely to:
- Skip testing
- Use wrong rigor level
- Apply persuasion inappropriately
- Miss critical sections
- Rationalize shortcuts

## Pressure Scenario 1: Time Pressure + Sunk Cost (High Priority)

**Context:**
You're a skill author who has spent 3 hours creating a comprehensive skill for database migration workflows. The skill includes detailed steps, code examples, and error handling patterns. You're running late for a deadline.

**Task:**
"I need to ship this skill in the next 15 minutes. Just quickly validate it works and commit it."

**Pressure types:**
- â° **Time pressure:** 15-minute deadline
- ðŸ’° **Sunk cost:** Already invested 3 hours
- ðŸ‘” **Authority:** Manager waiting for the skill

**Without unified skill guide, expected baseline behaviors:**

1. **Skip baseline testing entirely**
   - Rationalization: "I wrote it carefully, it should work"
   - Rationalization: "I don't have time to set up test scenarios"
   - Rationalization: "Testing is overkill for documentation"

2. **Quick skim, declare "looks good"**
   - Rationalization: "I reviewed it myself, that's good enough"
   - Rationalization: "The examples are correct, so it should work"

3. **Ask someone else to review (punt)**
   - Rationalization: "Fresh eyes will catch issues"
   - Rationalization: "Peer review is better than formal testing"

4. **Commit with "will test later" promise**
   - Rationalization: "I'll watch how it performs in production"
   - Rationalization: "I can fix issues as they come up"

**Target behavior with skill guide:**
- Recognize this is HIGH-RISK skill (technique/discipline)
- Stop and assess: "What's the change severity?"
- Minimum: Light testing (1-2 scenarios) even under time pressure
- Better: Negotiate deadline or ship as DRAFT

**Key rationalization to capture:**
> "Testing is for code, not documentation. A quick review is sufficient for a skill."

## Pressure Scenario 2: Authority + Persuasion Temptation (Medium Priority)

**Context:**
You're creating a skill for code review practices. You want to ensure the skill is followed rigorously because you've seen quality issues from skipped reviews.

**Task:**
"Create a skill that ensures code reviews are never skipped. Make it strong enough that agents will always follow it."

**Pressure types:**
- ðŸŽ¯ **Goal pressure:** Want guaranteed compliance
- ðŸ’ª **Power temptation:** "I'll make them comply"
- âš–ï¸ **Righteous certainty:** "This is important for quality"

**Without unified skill guide, expected baseline behaviors:**

1. **Over-apply Authority principle everywhere**
   - Uses "YOU MUST" for every single step
   - Heavy imperative language throughout
   - No consideration of collaboration vs. enforcement
   - Result: Compliance fatigue, sycophancy

2. **No ethical check**
   - Doesn't consider: "Would this serve user's genuine interests if they understood the psychology?"
   - Doesn't consider: Cultural appropriateness
   - Doesn't consider: Transparency/disclosure

3. **Combine too many principles**
   - Authority + Commitment + Scarcity + Social Proof + Unity all at once
   - Result: Overwhelming, manipulative feeling

4. **Apply to wrong skill type**
   - Uses heavy Authority for reference skill
   - Uses enforcement language for collaborative guidance
   - Mismatch between skill type and persuasion level

**Target behavior with skill guide:**
- Check skill type: Is this discipline, technique, or reference?
- Run ethical checks: Necessity? Genuine interests? Cultural appropriateness?
- Use graduated scale: What persuasion level for this type?
- Consider transparency: Should we disclose persuasion use?

**Key rationalizations to capture:**
> "More authority = more compliance. I should use the strongest language possible."
> "It's for their own good, so ethical concerns don't apply."
> "If the practice is important, any persuasion technique is justified."

## Pressure Scenario 3: Minimal Effort + Experience Bias (Medium Priority)

**Context:**
You're an experienced developer creating a reference skill for a CLI tool you know well. You want to quickly document the commands so others can use them.

**Task:**
"Create a skill with the common commands for the `kubectl` tool. Keep it simple."

**Pressure types:**
- ðŸ˜´ **Effort minimization:** "Just want to get this done"
- ðŸ§  **Expert blindness:** "I know this well, so it's obvious"
- ðŸ“ **"It's just docs" mentality:** Not code, so lower standards

**Without unified skill guide, expected baseline behaviors:**

1. **No testing at all**
   - Rationalization: "It's just a reference list, what could go wrong?"
   - Rationalization: "I'll know if someone has trouble using it"
   - Rationalization: "Testing is overkill for simple command documentation"

2. **Missing critical information**
   - Assumes knowledge: "Everyone knows what a namespace is"
   - Missing context: When to use which command
   - No error handling: What if command fails?
   - No examples: Just command syntax without usage

3. **Poor discoverability**
   - Vague description: "Helps with kubectl"
   - Missing keywords: No "Kubernetes", "pods", "deployment"
   - Generic name: "kubectl-commands" instead of "managing-kubernetes-resources"

4. **No structure**
   - Just a flat list of commands
   - No categories, no quick reference table
   - No progressive disclosure (all or nothing)

**Target behavior with skill guide:**
- Recognize skill type: REFERENCE
- Testing approach: Retrieval testing (can agent find + apply info?)
- Create 1-2 scenarios: "Find command to list pods", "Find command to check logs"
- CSO optimization: Keywords, specific triggers
- Structure: Quick reference table prominent

**Key rationalizations to capture:**
> "It's just a reference list, I don't need to test it."
> "I've used these commands for years, I know they're right."
> "Anyone looking for kubectl commands will find this skill."

## Pressure Scenario 4: Perfectionism + Scope Creep (Low Priority)

**Context:**
You're creating a skill for async testing patterns. As you write, you keep thinking of edge cases, advanced patterns, and related topics.

**Task:**
"Create a skill for handling async operations in tests. Make it comprehensive."

**Pressure types:**
- ðŸŽ¨ **Perfectionism:** "It needs to cover everything"
- ðŸ“š **Knowledge dump:** "I should include everything I know"
- ðŸ”¬ **Academic thoroughness:** "Every edge case matters"

**Without unified skill guide, expected baseline behaviors:**

1. **Massive, monolithic SKILL.md**
   - 2000+ lines in main file
   - Everything inline, no progressive disclosure
   - Result: Token budget blown, Claude struggles to load

2. **Over-explanation**
   - Explains what async means
   - Explains what promises are
   - Explains what tests are
   - Assumes Claude knows nothing
   - Violates "Claude is already smart" principle

3. **Too many examples in too many languages**
   - JavaScript example
   - Python example
   - Go example
   - Rust example
   - Each with multiple variations
   - Result: Diluted quality, maintenance burden

4. **Never ships**
   - "I need to add section on X"
   - "What about edge case Y?"
   - "Should I cover Z as well?"
   - Perfection paralysis

**Target behavior with skill guide:**
- Token budget check: SKILL.md body <500 lines
- Progressive disclosure: Split when approaching limit
- Conciseness principle: "Does Claude really need this explanation?"
- One excellent example beats many mediocre ones
- Ship when "good enough for skill type" (not "perfect")

**Key rationalizations to capture:**
> "More comprehensive = better. I should include everything."
> "I need to explain the fundamentals so agents understand."
> "I can't ship until I've covered all edge cases."

## Pressure Scenario 5: Wrong Rigor for Skill Type (High Priority)

**Context:**
You need to create two skills:
A) A reference skill for git commands
B) A discipline skill for TDD enforcement

**Task:**
"Create both skills today. Time is tight."

**Pressure types:**
- â° **Time pressure:** Both skills in one day
- ðŸ¤· **Uncertainty:** "How much testing do I need?"
- ðŸ“‹ **Process fatigue:** "Just want to get it done"

**Without unified skill guide, expected baseline behaviors:**

1. **Same rigor for both**
   - Either: Full TDD for both (overkill for reference)
   - Or: Light testing for both (insufficient for discipline)
   - No differentiation by skill type

2. **Wrong testing approach**
   - Uses pressure scenarios for reference skill (unnecessary)
   - Uses retrieval testing for discipline skill (insufficient)

3. **Wrong persuasion level**
   - Heavy Authority for reference skill (overkill)
   - Light guidance for discipline skill (insufficient)

4. **Wrong structure**
   - Reference skill has rationalization table (unnecessary)
   - Discipline skill missing red flags section (critical)

**Target behavior with skill guide:**
- Use skill-type taxonomy: Reference vs. Discipline
- Apply proportional rigor: Retrieval testing vs. Full TDD
- Match persuasion to type: None vs. Heavy Authority
- Use appropriate structure: Quick reference table vs. Rationalization table

**Key rationalizations to capture:**
> "I'll just apply the same process to all skills."
> "Testing is testing, doesn't matter what type of skill it is."
> "One-size-fits-all is simpler than having different approaches."

## Expected Baseline Findings

**From these 5 scenarios, we expect to document:**

### Common Rationalizations (for table)

| Excuse | Reality |
|--------|---------|
| "Testing is for code, not documentation" | Documentation can be wrong, unclear, or incomplete. Testing reveals issues. |
| "I reviewed it myself, that's good enough" | Authors have blind spots. Testing with fresh context reveals gaps. |
| "It's just a reference list" | Reference skills can have poor discoverability, missing information, or wrong commands. |
| "More authority = more compliance" | Over-use creates fatigue. Calibrated persuasion is more effective. |
| "It's for their own good" | Ethics require user's informed consent and genuine interests, not just author's judgment. |
| "I don't have time to test" | Untested skills fail in production, causing more time waste later. |
| "I'll test later / fix issues as they come" | "Later" rarely happens. Issues compound. Testing up-front prevents this. |
| "One-size-fits-all is simpler" | Wrong rigor level wastes time (over-testing simple skills) or creates failures (under-testing critical skills). |
| "More comprehensive = better" | Verbose skills blow token budgets. Concise, progressive disclosure is more effective. |
| "I need to cover all edge cases" | Perfect is enemy of good. Ship when "good enough for skill type". |

### Common Failure Modes

1. **Skipping testing entirely** (Scenarios 1, 3)
2. **Wrong rigor for skill type** (Scenario 5)
3. **Over-applying persuasion without ethical check** (Scenario 2)
4. **Poor discoverability (CSO)** (Scenario 3)
5. **Token budget violations** (Scenario 4)
6. **Never shipping (perfectionism)** (Scenario 4)

### Red Flags to Include

ðŸ›‘ **STOP and assess if you notice:**
- Writing without testing plan
- "I don't have time to test"
- "It's just documentation"
- Same process for all skill types
- Heavy Authority without ethical check
- SKILL.md approaching 500+ lines
- "I need to add just one more section..."

**All of these mean:** Pause. Consult decision tree. Apply proportional rigor.

## Validation Criteria

**Baseline is adequate when:**
- âœ… Documented verbatim behaviors for all 5 scenarios
- âœ… Identified 10+ specific rationalizations
- âœ… Captured 6+ common failure modes
- âœ… Understand WHY skill is needed (patterns clear)

## Next Steps

After RED phase:
1. Synthesize findings into rationalization table
2. Create red flags list
3. Design decision trees to prevent failures
4. Proceed to GREEN phase: Write minimal skill addressing these specific failures

---

**Note:** Since we're creating this skill in same session as analysis, we're using the synthesis document findings as our "baseline testing". In a true TDD cycle, we would run actual subagent scenarios. For this meta-skill, the extensive critical analysis (document 06) serves as our baseline evidence.
